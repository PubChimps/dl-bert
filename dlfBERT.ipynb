{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "AG1Orzhch3sF",
    "outputId": "3df2701a-b59d-45dc-ed1a-063062b70f9f"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBaGZWhRiEI7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import Dict, List, Tuple\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from transformers.data.metrics import acc_and_f1, simple_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "id": "2eP7-9-aiEym",
    "outputId": "40acd3b8-f9b1-462b-e47c-398bea8ccec3"
   },
   "outputs": [],
   "source": [
    "!wget -nc -O ds.npy https://ibm.box.com/shared/static/o9x8cglra6bpvngt537nlo7pe7ctjrk6.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "368ef47f34c94a1a827cc6f26913f047",
      "459d696b9a6843538876f7d2bf403c5a",
      "f025a2aac7534c73b3cb0b1761367ffa",
      "3be2201e16de4f3a8be6cc36d25bab77",
      "9d12f9ec5cf4491ab94447cbd18dcb3f",
      "f62aef0105af4b9a81659e4d097dc5f4",
      "062e95d3643e4a5ebec1a2a66fae283f",
      "37dfc0ffdf1f4034aa707dce4bea2310",
      "1a33b5e2b0764996a8b3a9614b0d2ea1",
      "36f0518c5c074ed6b5399ea051419370",
      "d71a887442aa4d4c8c6d5d0e3e0a5756",
      "8c603c1235824e95aa02d08c3701385e",
      "b48dfe72754b4e29b32a0b25c57b318a",
      "cf24bc1385d64100a6dbf0e46e394824",
      "4dfe5220ef104708b4f059ce90c808a8",
      "c3bfcb849da34c50b4b63f0683a06e39",
      "2be8a0e0a14a4ca2b227ffb646e3e65f",
      "fa87c2693ab84f05afee67b708277e3f",
      "ce4ae7aae04948c3a211f2b74ec034ae",
      "db816723fa99462e9b2a2196e85fc465",
      "ad0b618e498a488c8a5bef192b37bdbe",
      "1d4e5a54af424d3eae3e66d4f47d67ed",
      "47f9855a22344ac6adbe00ed14f9e76a",
      "7d6ee92d950d497a9034df6f7d2913b1",
      "e2abf56763da4594b516a12bcc6bc53d",
      "58aac3e616ce44e5a9db060bfcc8e8ad",
      "a311bf310e364683a11d62cea0aa2ad0",
      "d7d050aa975241339cf6aa9eb136fd18",
      "c66c6cb229a4487b987a2b37f0bed906",
      "25fe9e38b81d4560a5f8fd0836b96195",
      "54ab529118d946c0a91cfda892c45576",
      "85b1f82a023f429ca121b666bee7bb26",
      "34754ac3b9554f7a8adb0b3392c2b8e7",
      "cf8f2cdb115e4050943b5decfe8e18ff",
      "80faea1e9c344aedb3440f6dfaf440b3",
      "42482f00eed14f1f92b3fc9980ebc06b",
      "ccf58e93ce9041bba78260e594805a81",
      "f87344b3afbe446aa2cf9780cbcac56a",
      "0d43ec31531e4d51b4146343c43a5b4b",
      "80edda27ab2b479eb55e16c4aa930e77"
     ]
    },
    "colab_type": "code",
    "id": "du_1V5MLiGwr",
    "outputId": "7af9848f-21c9-4192-c20b-1f9435a8b532"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "tb_writer = SummaryWriter()\n",
    "EVALUATE = True\n",
    "\n",
    "LANGUAGES = [\n",
    "    \"tensorflow\",\n",
    "    \"pytorch\"\n",
    "]\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"huggingface/CodeBERTa-small-v1\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"huggingface/CodeBERTa-small-v1\", num_labels=len(LANGUAGES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3DHXKDQiUzQ"
   },
   "outputs": [],
   "source": [
    "class CodeSearchNetDataset(Dataset):\n",
    "    examples: List[Tuple[List[int], int]]\n",
    "\n",
    "    def __init__(self, data):\n",
    "\n",
    "        self.examples = []\n",
    "\n",
    "        lines = []\n",
    "        for code in tqdm(data):\n",
    "            if code[0] == '':\n",
    "                continue\n",
    "            label = code[1]\n",
    "            label_idx = LANGUAGES.index(label)\n",
    "            examples = [(tokenizer.encode(code[0], max_length=512), label_idx)]\n",
    "            self.examples += examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Weâ€™ll pad at the batch level.\n",
    "        return self.examples[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8dvNhncibTT"
   },
   "outputs": [],
   "source": [
    "ds = np.load('ds.npy', allow_pickle = True)\n",
    "np.random.shuffle(ds)\n",
    "dstrain = ds[:int(len(ds)*.88)]\n",
    "dstest = ds[int(len(ds)*.88):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "9mbTuNZ1idjZ",
    "outputId": "b99cd265-30f2-4fb0-dc98-97cbc43b739d"
   },
   "outputs": [],
   "source": [
    "train_dataset = CodeSearchNetDataset(dstrain)\n",
    "test_dataset = CodeSearchNetDataset(dstest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JkEUO42JigPF",
    "outputId": "226cbf7a-82d8-4d54-d6a8-1394e664d23f"
   },
   "outputs": [],
   "source": [
    "def collate(examples):\n",
    "    input_ids = pad_sequence([torch.tensor(x[0]) for x in examples], batch_first=True, padding_value=1)\n",
    "    labels = torch.tensor([x[1] for x in examples])\n",
    "    return input_ids, labels\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "model.to(\"cuda\")\n",
    "model.train()\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "## ^^ Only train final layer.\n",
    "\n",
    "print(f\"num params:\", model.num_parameters())\n",
    "print(f\"num trainable params:\", model.num_parameters(only_trainable=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5vfJskdhikS-",
    "outputId": "787bea6f-3b1d-423d-ff56-6fffd18db550"
   },
   "outputs": [],
   "source": [
    "def evaluate(best):\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = np.empty((0), dtype=np.int64)\n",
    "    out_label_ids = np.empty((0), dtype=np.int64)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    eval_dataloader = DataLoader(test_dataset, batch_size=128, collate_fn=collate)\n",
    "    for step, (input_ids, labels) in enumerate(tqdm(eval_dataloader, desc=\"Eval\")):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids.to(\"cuda\"), labels=labels.to(\"cuda\"))\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            eval_loss += loss.mean().item()\n",
    "            nb_eval_steps += 1\n",
    "        preds = np.append(preds, logits.argmax(dim=1).detach().cpu().numpy(), axis=0)\n",
    "        out_label_ids = np.append(out_label_ids, labels.detach().cpu().numpy(), axis=0)\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    acc = simple_accuracy(preds, out_label_ids)\n",
    "    f1 = f1_score(y_true=out_label_ids, y_pred=preds, average=\"macro\")\n",
    "    print(\"=== Eval: loss ===\", eval_loss)\n",
    "    print(\"=== Eval: acc. ===\", acc)\n",
    "    print(\"=== Eval: f1 ===\", f1)\n",
    "    if acc > best:\n",
    "      best = round(acc,2)\n",
    "      os.mkdir('./best' + str(best))\n",
    "      print(\"=== Saving model ===\")\n",
    "      model.save_pretrained('./best' + str(best))\n",
    "    # print(acc_and_f1(preds, out_label_ids))\n",
    "    tb_writer.add_scalars(\"eval\", {\"loss\": eval_loss, \"acc\": acc, \"f1\": f1}, global_step)\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "### Training loop\n",
    "\n",
    "global_step = 0\n",
    "best = 0\n",
    "train_iterator = trange(0, 10, desc=\"Epoch\")\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, (input_ids, labels) in enumerate(epoch_iterator):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids.to(\"cuda\"), labels=labels.to(\"cuda\"))\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        tb_writer.add_scalar(\"training_loss\", loss.item(), global_step)\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "        if EVALUATE and global_step % 50 == 0:\n",
    "            best = evaluate(best)\n",
    "            model.train()\n",
    "\n",
    "\n",
    "evaluate(best)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "dlfBERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
