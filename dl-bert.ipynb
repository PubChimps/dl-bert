{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3a2cc2b70884d33a883316d2a0d1f61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0b48c09011fc4178bc1b6f69b98b9e16",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1c0a347e3b2f4185b0eea91632763f56",
              "IPY_MODEL_9afc4a725c8d412b9f384c4f8415e22e"
            ]
          }
        },
        "0b48c09011fc4178bc1b6f69b98b9e16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c0a347e3b2f4185b0eea91632763f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e8b4b63df27f4504b0ccf20cf26e06c0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 336407488,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 336407488,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_59b00f52c5254dc4b785eb6af761a256"
          }
        },
        "9afc4a725c8d412b9f384c4f8415e22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d1756ebe78c34bf091e283203f9d6408",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 336M/336M [00:06&lt;00:00, 53.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bf638c4a0b7641a1a9812cfbd70a90f0"
          }
        },
        "e8b4b63df27f4504b0ccf20cf26e06c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "59b00f52c5254dc4b785eb6af761a256": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d1756ebe78c34bf091e283203f9d6408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bf638c4a0b7641a1a9812cfbd70a90f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZcT38HHbdNP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "b730bdda-70bb-4c0e-a985-2bf16ba767cf"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 8.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 24.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 53.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=0f7f3ba3555d09ef8a5bc0cc0af81fc81bebe23da7ef244408579b08dba7d3bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.1\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.7.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmECsh6hb-VH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from typing import Dict, List, Tuple\n",
        "from torch.utils.tensorboard.writer import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAmXgzhpb-up",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "bca6e753-4c94-4428-9887-6c15d7515efe"
      },
      "source": [
        "!wget -nc -O ds.npy https://ibm.box.com/shared/static/o9x8cglra6bpvngt537nlo7pe7ctjrk6.npy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-18 01:40:02--  https://ibm.box.com/shared/static/o9x8cglra6bpvngt537nlo7pe7ctjrk6.npy\n",
            "Resolving ibm.box.com (ibm.box.com)... 185.235.236.197\n",
            "Connecting to ibm.box.com (ibm.box.com)|185.235.236.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/o9x8cglra6bpvngt537nlo7pe7ctjrk6.npy [following]\n",
            "--2020-05-18 01:40:03--  https://ibm.box.com/public/static/o9x8cglra6bpvngt537nlo7pe7ctjrk6.npy\n",
            "Reusing existing connection to ibm.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://ibm.ent.box.com/public/static/o9x8cglra6bpvngt537nlo7pe7ctjrk6.npy [following]\n",
            "--2020-05-18 01:40:03--  https://ibm.ent.box.com/public/static/o9x8cglra6bpvngt537nlo7pe7ctjrk6.npy\n",
            "Resolving ibm.ent.box.com (ibm.ent.box.com)... 185.235.236.211\n",
            "Connecting to ibm.ent.box.com (ibm.ent.box.com)|185.235.236.211|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!7SKUWFSdiUT8deulkmPtW3KLXstczmiqlE4r06sPF2yg-yZJYG5TatlUbG3Hgx2BB962bvvBG8i65nnTHLvXHC7skJlhFxXfQmG88tACFOBO78uWSpM8ZFEPEDyUuAm1DnPl05iMFDh11MbODJa35M_rMrZreeDKulRFjjTR6e3DMqmAxxK7Je6-vKBS8BoOCuuRG2B5Uc8pKj7KDSCJtmSPTknWE4PvRFonR9RSANKWj_XdZ-yE-IX-a8fZ7B7AgpIlaoVrkwUGPLZsDZ2IpVUdLwotyTi1YVGmf8JcOgFU3h4dr7PdAHO_iyBl1s5vQVJX6VHtJ0tltzWS5LYaCDg_EoBtOe2YYVS3FPHyerjHYLTapT-dZlU1cID1Wu9Dka9O_0wWHPYDSswZv-9Fcm0ZVwT6cVxCDjob4EkweQH20UyvCcIaxt1rdMFdiNlsCs84NMQouFTJwu-YjiCBzsAP64InWSgroBwBJm6goMVHA4fq-6sZdFUa7yhV5VDS21r08Lnk-INGBrQyHLA51Q_TVwTm4eyhX2aw_y1Yc1oXpPhd2pC71PqU0Aqqt3_05FD1yS6kPlYEfgtwURiZqUgzzg5zrnU4rU6-_ZpwPGiBKtvrJUv_EIy_zNLdkEYY2UdCLNE7_ze4_UPqcVjzc9ZHoKsPdBUWnyybO99x0MK41gtkYzsG8GaBG_2SWXiQXy_2v19Xm-Svn8NdotZJezqnUQ-ITZA2y86RjJ_zH5ov9LQXiP_UtkCAp4IYWGLkaHgtjJIMYc4mUv_hsS62d2xddcIlI64t9BuwKwstZrBdZMs12-6DCzBeIsR24S2Dc6nmNKHtJP7qcuIVVJPZCN9MB-b4_EFgCUzlXfK_iZBIor-S0ZTjoaVXQn7D53-S6KPa2YmCmfTHC4n6TFxF_GEubISV4paKkgtBzimSr0chd79uKenoscOl2sJamzPyOZ7R6-lf1oGSmkhNKYh32qIHf9GzaLb5krxmoXSkJJnDV9TxJA-TrJySfeFtTZCg9iHrZq8HJnyKwEfDpO-mdHFEaxmz69QDPZj462DiQQyFNX4paU9_feEm0SJvPn3nMeyg496XcxF6HjbQG8TjU4W_rTHJh9Db-tEsN4z5A0JpPyk0JT4It6VWo5vAHscS2QLvmjM74KPmkH1sibPYeWw4U2xnp4vGojz7O1hrncc6_WElhZKEE8WlyeHh4d6TJWAimeMSeYPn9gAvPoFVATG9p-x8pop1sVxg1XYaLM0wW0-E-Hp4s6Tgtm8Key8fVlndNtpTShcbr74_XGAqGzj1YQ_N1Txs15r7WDqf4kn1ZR9iK6Dv2aAcX4vCftz3GHqs-spPRZnS9zGlF9j9w1DXlw7SY2zXkPd_pFdccSdc_Js8ko9M00kjxLbL7xOahw../download [following]\n",
            "--2020-05-18 01:40:03--  https://public.boxcloud.com/d/1/b1!7SKUWFSdiUT8deulkmPtW3KLXstczmiqlE4r06sPF2yg-yZJYG5TatlUbG3Hgx2BB962bvvBG8i65nnTHLvXHC7skJlhFxXfQmG88tACFOBO78uWSpM8ZFEPEDyUuAm1DnPl05iMFDh11MbODJa35M_rMrZreeDKulRFjjTR6e3DMqmAxxK7Je6-vKBS8BoOCuuRG2B5Uc8pKj7KDSCJtmSPTknWE4PvRFonR9RSANKWj_XdZ-yE-IX-a8fZ7B7AgpIlaoVrkwUGPLZsDZ2IpVUdLwotyTi1YVGmf8JcOgFU3h4dr7PdAHO_iyBl1s5vQVJX6VHtJ0tltzWS5LYaCDg_EoBtOe2YYVS3FPHyerjHYLTapT-dZlU1cID1Wu9Dka9O_0wWHPYDSswZv-9Fcm0ZVwT6cVxCDjob4EkweQH20UyvCcIaxt1rdMFdiNlsCs84NMQouFTJwu-YjiCBzsAP64InWSgroBwBJm6goMVHA4fq-6sZdFUa7yhV5VDS21r08Lnk-INGBrQyHLA51Q_TVwTm4eyhX2aw_y1Yc1oXpPhd2pC71PqU0Aqqt3_05FD1yS6kPlYEfgtwURiZqUgzzg5zrnU4rU6-_ZpwPGiBKtvrJUv_EIy_zNLdkEYY2UdCLNE7_ze4_UPqcVjzc9ZHoKsPdBUWnyybO99x0MK41gtkYzsG8GaBG_2SWXiQXy_2v19Xm-Svn8NdotZJezqnUQ-ITZA2y86RjJ_zH5ov9LQXiP_UtkCAp4IYWGLkaHgtjJIMYc4mUv_hsS62d2xddcIlI64t9BuwKwstZrBdZMs12-6DCzBeIsR24S2Dc6nmNKHtJP7qcuIVVJPZCN9MB-b4_EFgCUzlXfK_iZBIor-S0ZTjoaVXQn7D53-S6KPa2YmCmfTHC4n6TFxF_GEubISV4paKkgtBzimSr0chd79uKenoscOl2sJamzPyOZ7R6-lf1oGSmkhNKYh32qIHf9GzaLb5krxmoXSkJJnDV9TxJA-TrJySfeFtTZCg9iHrZq8HJnyKwEfDpO-mdHFEaxmz69QDPZj462DiQQyFNX4paU9_feEm0SJvPn3nMeyg496XcxF6HjbQG8TjU4W_rTHJh9Db-tEsN4z5A0JpPyk0JT4It6VWo5vAHscS2QLvmjM74KPmkH1sibPYeWw4U2xnp4vGojz7O1hrncc6_WElhZKEE8WlyeHh4d6TJWAimeMSeYPn9gAvPoFVATG9p-x8pop1sVxg1XYaLM0wW0-E-Hp4s6Tgtm8Key8fVlndNtpTShcbr74_XGAqGzj1YQ_N1Txs15r7WDqf4kn1ZR9iK6Dv2aAcX4vCftz3GHqs-spPRZnS9zGlF9j9w1DXlw7SY2zXkPd_pFdccSdc_Js8ko9M00kjxLbL7xOahw../download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 107.152.26.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|107.152.26.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7531582344 (7.0G) [application/octet-stream]\n",
            "Saving to: ‘ds.npy’\n",
            "\n",
            "ds.npy              100%[===================>]   7.01G  26.8MB/s    in 4m 33s  \n",
            "\n",
            "2020-05-18 01:44:37 (26.4 MB/s) - ‘ds.npy’ saved [7531582344/7531582344]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb6MQBl-cBSb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715,
          "referenced_widgets": [
            "e3a2cc2b70884d33a883316d2a0d1f61",
            "0b48c09011fc4178bc1b6f69b98b9e16",
            "1c0a347e3b2f4185b0eea91632763f56",
            "9afc4a725c8d412b9f384c4f8415e22e",
            "e8b4b63df27f4504b0ccf20cf26e06c0",
            "59b00f52c5254dc4b785eb6af761a256",
            "d1756ebe78c34bf091e283203f9d6408",
            "bf638c4a0b7641a1a9812cfbd70a90f0"
          ]
        },
        "outputId": "cedf5e81-6aed-4e40-a5c7-d2d41e2f7429"
      },
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "tb_writer = SummaryWriter()\n",
        "EVALUATE = True\n",
        "\n",
        "LANGUAGES = [\n",
        "    \"tensorflow\",\n",
        "    \"pytorch\"\n",
        "]\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"huggingface/CodeBERTa-small-v1\")\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"huggingface/CodeBERTa-small-v1\", num_labels=len(LANGUAGES))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.tokenization_utils:Model name 'huggingface/CodeBERTa-small-v1' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'huggingface/CodeBERTa-small-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/huggingface/CodeBERTa-small-v1/vocab.json from cache at /root/.cache/torch/transformers/4378e0cbd4e198a29d18acffb589430ff433fbbfd637803fddbc8e4b5108368f.516f21a3e3c538a6e0b52aea5987651b995c82fba6021ab1cd79ef987877432b\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/huggingface/CodeBERTa-small-v1/merges.txt from cache at /root/.cache/torch/transformers/2b5cb186465fac4f5d94ce950b4b26ba8333b9387ec72e11872819344c529f7e.cdee6ff5de424ca98962369a559ecdef682286d4dfc7a37d91fd6a4cc1896f4a\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/huggingface/CodeBERTa-small-v1/added_tokens.json from cache at None\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/huggingface/CodeBERTa-small-v1/special_tokens_map.json from cache at None\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/huggingface/CodeBERTa-small-v1/tokenizer_config.json from cache at /root/.cache/torch/transformers/1b83cbda23054c9cf5dd1b6cdb179164fec7d3f1bc4aa926da97db3836186aa9.02b17bf1daddea30357807577d0b4159277ee522b038c41800666fc8d88d1fee\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/huggingface/CodeBERTa-small-v1/config.json from cache at /root/.cache/torch/transformers/f6d3d6a640654d75953c1d5d7552a54ee4e80573e7d0d517c14038a9c3d18459.2fa9ab01710558b67d737fc4bc74f603992a838000e2218d632311a52eb3a88a\n",
            "INFO:transformers.configuration_utils:Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "INFO:filelock:Lock 140278814953936 acquired on /root/.cache/torch/transformers/8df1efcdb462fb48d8e3d93d8f869641fce29d289d95b8d357a5854c7dc2b1b9.5561c147dbc48bf85cff1b96abd42419658212abeb129931ca149d3ad567a7d4.lock\n",
            "INFO:transformers.file_utils:https://cdn.huggingface.co/huggingface/CodeBERTa-small-v1/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpg3v6ih49\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3a2cc2b70884d33a883316d2a0d1f61",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=336407488.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:storing https://cdn.huggingface.co/huggingface/CodeBERTa-small-v1/pytorch_model.bin in cache at /root/.cache/torch/transformers/8df1efcdb462fb48d8e3d93d8f869641fce29d289d95b8d357a5854c7dc2b1b9.5561c147dbc48bf85cff1b96abd42419658212abeb129931ca149d3ad567a7d4\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/8df1efcdb462fb48d8e3d93d8f869641fce29d289d95b8d357a5854c7dc2b1b9.5561c147dbc48bf85cff1b96abd42419658212abeb129931ca149d3ad567a7d4\n",
            "INFO:filelock:Lock 140278814953936 released on /root/.cache/torch/transformers/8df1efcdb462fb48d8e3d93d8f869641fce29d289d95b8d357a5854c7dc2b1b9.5561c147dbc48bf85cff1b96abd42419658212abeb129931ca149d3ad567a7d4.lock\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/huggingface/CodeBERTa-small-v1/pytorch_model.bin from cache at /root/.cache/torch/transformers/8df1efcdb462fb48d8e3d93d8f869641fce29d289d95b8d357a5854c7dc2b1b9.5561c147dbc48bf85cff1b96abd42419658212abeb129931ca149d3ad567a7d4\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.modeling_utils:Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "INFO:transformers.modeling_utils:Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvR_fRTgdY06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CodeSearchNetDataset(Dataset):\n",
        "    examples: List[Tuple[List[int], int]]\n",
        "\n",
        "    def __init__(self, data):\n",
        "        \"\"\"\n",
        "        train or test data\n",
        "        \"\"\"\n",
        "\n",
        "        self.examples = []\n",
        "\n",
        "        lines = []\n",
        "        for code in tqdm(data):\n",
        "            if code[0] == '':\n",
        "                continue\n",
        "            label = code[1]\n",
        "            label_idx = LANGUAGES.index(label)\n",
        "            examples = [(tokenizer.encode(code[0], max_length=512), label_idx)]\n",
        "            self.examples += examples\n",
        "        print(\"🔥🔥\")\n",
        "        \n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # We’ll pad at the batch level.\n",
        "        return self.examples[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiRK_h8vddSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds = np.load('ds.npy', allow_pickle = True)\n",
        "dstrain = ds[:int(len(ds)*.88)]\n",
        "dstest = ds[int(len(ds)*.88):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-TLH-PodfyX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "554b1898-db79-470c-e4e7-22dc77bc5786"
      },
      "source": [
        "train_dataset = CodeSearchNetDataset(dstrain)\n",
        "test_dataset = CodeSearchNetDataset(dstest)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10606/10606 [02:38<00:00, 67.03it/s]\n",
            "  0%|          | 5/1447 [00:00<00:47, 30.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "🔥🔥\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1447/1447 [00:22<00:00, 65.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "🔥🔥\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTW0mArAeXMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2a2d27df-19e0-405e-c228-7205e2af1006"
      },
      "source": [
        "def collate(examples):\n",
        "    input_ids = pad_sequence([torch.tensor(x[0]) for x in examples], batch_first=True, padding_value=1)\n",
        "    labels = torch.tensor([x[1] for x in examples])\n",
        "    return input_ids, labels\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, collate_fn=collate)\n",
        "\n",
        "batch = next(iter(train_dataloader))\n",
        "\n",
        "model.to(\"cuda\")\n",
        "model.train()\n",
        "for param in model.roberta.parameters():\n",
        "    param.requires_grad = False\n",
        "## ^^ Only train final layer.\n",
        "\n",
        "print(f\"num params:\", model.num_parameters())\n",
        "print(f\"num trainable params:\", model.num_parameters(only_trainable=True))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num params: 84043010\n",
            "num trainable params: 592130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo28hg8efUdS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1cca2881-f57c-499c-8a8c-ad069f4c979e"
      },
      "source": [
        "def evaluate():\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    preds = np.empty((0), dtype=np.int64)\n",
        "    out_label_ids = np.empty((0), dtype=np.int64)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    eval_dataloader = DataLoader(test_dataset, batch_size=512, collate_fn=collate)\n",
        "    for step, (input_ids, labels) in enumerate(tqdm(eval_dataloader, desc=\"Eval\")):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids.to(\"cuda\"), labels=labels.to(\"cuda\"))\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[1]\n",
        "            eval_loss += loss.mean().item()\n",
        "            nb_eval_steps += 1\n",
        "        preds = np.append(preds, logits.argmax(dim=1).detach().cpu().numpy(), axis=0)\n",
        "        out_label_ids = np.append(out_label_ids, labels.detach().cpu().numpy(), axis=0)\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    acc = simple_accuracy(preds, out_label_ids)\n",
        "    f1 = f1_score(y_true=out_label_ids, y_pred=preds, average=\"macro\")\n",
        "    print(\"=== Eval: loss ===\", eval_loss)\n",
        "    print(\"=== Eval: acc. ===\", acc)\n",
        "    print(\"=== Eval: f1 ===\", f1)\n",
        "    # print(acc_and_f1(preds, out_label_ids))\n",
        "    tb_writer.add_scalars(\"eval\", {\"loss\": eval_loss, \"acc\": acc, \"f1\": f1}, global_step)\n",
        "\n",
        "\n",
        "### Training loop\n",
        "\n",
        "global_step = 0\n",
        "train_iterator = trange(0, 4, desc=\"Epoch\")\n",
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "for _ in train_iterator:\n",
        "    epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
        "    for step, (input_ids, labels) in enumerate(epoch_iterator):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids.to(\"cuda\"), labels=labels.to(\"cuda\"))\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        tb_writer.add_scalar(\"training_loss\", loss.item(), global_step)\n",
        "        optimizer.step()\n",
        "        global_step += 1\n",
        "        if EVALUATE and global_step % 50 == 0:\n",
        "            evaluate()\n",
        "            model.train()\n",
        "\n",
        "\n",
        "evaluate()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/42 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 1/42 [00:05<03:35,  5.25s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 2/42 [00:10<03:25,  5.15s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 3/42 [00:15<03:18,  5.08s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 4/42 [00:20<03:11,  5.05s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 5/42 [00:25<03:06,  5.04s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 6/42 [00:30<03:02,  5.06s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 7/42 [00:35<02:58,  5.09s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 8/42 [00:40<02:54,  5.13s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 9/42 [00:45<02:49,  5.15s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 10/42 [00:51<02:45,  5.18s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 11/42 [00:56<02:42,  5.24s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 12/42 [01:01<02:39,  5.31s/it]\u001b[A\n",
            "Iteration:  31%|███       | 13/42 [01:07<02:36,  5.39s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 14/42 [01:13<02:32,  5.46s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 15/42 [01:18<02:30,  5.58s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 16/42 [01:24<02:26,  5.64s/it]\u001b[A\n",
            "Iteration:  40%|████      | 17/42 [01:30<02:23,  5.72s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 18/42 [01:36<02:20,  5.85s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 19/42 [01:42<02:17,  5.96s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 20/42 [01:49<02:14,  6.09s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 21/42 [01:55<02:10,  6.23s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 22/42 [02:02<02:06,  6.32s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 23/42 [02:08<02:00,  6.35s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 24/42 [02:15<01:54,  6.33s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 25/42 [02:21<01:47,  6.30s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 26/42 [02:27<01:40,  6.27s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 27/42 [02:33<01:33,  6.23s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 28/42 [02:39<01:26,  6.19s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 29/42 [02:45<01:20,  6.17s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 30/42 [02:52<01:13,  6.16s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 31/42 [02:58<01:07,  6.18s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 32/42 [03:04<01:01,  6.19s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 33/42 [03:10<00:55,  6.22s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 34/42 [03:17<00:49,  6.24s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 35/42 [03:23<00:43,  6.27s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 36/42 [03:29<00:37,  6.29s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 37/42 [03:36<00:31,  6.29s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 38/42 [03:42<00:25,  6.27s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 39/42 [03:48<00:18,  6.25s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 40/42 [03:54<00:12,  6.23s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 41/42 [04:00<00:06,  6.21s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 42/42 [04:03<00:00,  5.79s/it]\n",
            "Epoch:  25%|██▌       | 1/4 [04:03<12:09, 243.11s/it]\n",
            "Iteration:   0%|          | 0/42 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 1/42 [00:06<04:15,  6.22s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 2/42 [00:12<04:09,  6.24s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 3/42 [00:18<04:04,  6.26s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 4/42 [00:25<03:57,  6.26s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 5/42 [00:31<03:52,  6.27s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 6/42 [00:37<03:46,  6.28s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 7/42 [00:43<03:39,  6.28s/it]\u001b[A\n",
            "\n",
            "Eval:   0%|          | 0/3 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-0f8b080704d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mEVALUATE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-0f8b080704d3>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         )\n\u001b[1;32m    346\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m         )\n\u001b[1;32m    738\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             )\n\u001b[1;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     ):\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mself_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add self attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    312\u001b[0m     ):\n\u001b[1;32m    313\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         )\n\u001b[1;32m    316\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;31m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 6.00 GiB (GPU 0; 14.73 GiB total capacity; 9.33 GiB already allocated; 3.75 GiB free; 10.10 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYq2Lyczfd7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}